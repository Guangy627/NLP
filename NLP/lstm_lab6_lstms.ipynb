{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2263c298",
   "metadata": {},
   "source": [
    "# Lab 6: LSTM for POS tagging\n",
    "\n",
    "Long Short-Term Memory networks (LSTMs) are a variant of Recurrent Neural Networks (RNNs).\n",
    "\n",
    "RNNs represent the prior context using recurrent network connections between some of their nodes, so that output from these nodes affects the subsequent input to the same nodes.\n",
    "\n",
    "However, RNNs are infamous for the vanishing gradient problem. LSTMs deal with this problem by using gates that control the flow of information into and out of the units of its layers.\n",
    "\n",
    "In today's lab, we will be working on a part-of-speech (POS) tagging task using an LSTM network.\n",
    "More specifically:\n",
    "1) We will work on a simple LSTM network. You can experiment with different values for:\n",
    "   * word embedding and hidden layer dimensions\n",
    "   * number of training epochs\n",
    "2) We will use a 10% sample of the Penn Treebank corpus as training set.\n",
    "3) We will perform some simple preprocessing on our training data in order to transform them into word embeddings for our LSTM network. You are free to experiment further, e.g. lowercasing the text, removing punctuation, etc.\n",
    "4) We will finally test our LSTM POS tagger in a few sentences by comparing its predictions to the ones of the NLTK POS tagger.\n",
    "\n",
    "You may need to install a few libraries first (there are relevant comments in the cells that potentially require installation of some libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f0a1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112309330>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install pytorch by commenting out the next line:\n",
    "# !pip install torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # network layers and loss functions\n",
    "import torch.nn.functional as F # log_softmax function\n",
    "import torch.optim as optim # optimisation algorithm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d876e",
   "metadata": {},
   "source": [
    "### Load the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bd79f",
   "metadata": {},
   "source": [
    "We will work on a subset of the Penn Treebank dataset for POS tagging that is available through NLTK.\n",
    "Each sentence in ```treebank_chunk``` (the dataset) is represented as a list of tuples, each tuple consisting of a word and its POS tag.\n",
    "\n",
    "In particular, the first sentence of the dataset looks like this:\n",
    "\n",
    "```[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]```\n",
    "\n",
    "[Here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) is a list of the Penn Treebank POS tags.\n",
    "\n",
    "We want to tranform each sentence into a tuple that consists of a list of the sentence's words and a list of their corresponding POS tags.\n",
    "\n",
    "This is the purpose of the function ```prepare_treebank_data``` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f9b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk\n",
    "# !pip install nltk\n",
    "\n",
    "from nltk.corpus import treebank_chunk  # we will use treebank_chunk.tagged_sents()\n",
    "\n",
    "treebank_data = []\n",
    "\n",
    "# Represent each sentence found in treebank_chunk as a tuple,\n",
    "# consisting of a list of words and a list of corresponding POS tags\n",
    "\n",
    "def prepare_treebank_data(data):\n",
    "    for sentence in data:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for word, tag in sentence:\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "        treebank_data.append((words, tags))\n",
    "    return treebank_data\n",
    "\n",
    "\n",
    "# load our training data as list of sentences\n",
    "# each sentence is a tuple\n",
    "# each tuple contains a list of the sentence's words, and a list of their corresponding POS tags.\n",
    "\n",
    "training_data = prepare_treebank_data(treebank_chunk.tagged_sents())\n",
    "\n",
    "# create our vocabulary\n",
    "vocabulary = set()\n",
    "for words, _ in training_data:\n",
    "    vocabulary.update(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2f251",
   "metadata": {},
   "source": [
    "You can run ```help(treebank_chunk)``` to see what other methods are available apart from ```tagged_sents```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec6e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99f1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(treebank_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354815c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682ee71",
   "metadata": {},
   "source": [
    "Now that we have loaded our training set, we need to preprocess each sentence before it is passed as input to the LSTM.\n",
    "\n",
    "Every word must be assigned with a unique identifier (i.e., an integer value). These identifiers should then be used to represent each sentence. For example, the sentence ```That day was a wonderful day``` could be represented as ```309 12 5 0 98 12```.\n",
    "\n",
    "Finally, we have to transform these numerical values into tensor values, using ```torch.tensor(idxs, dtype=torch.long)```, where ```idxs``` is a list of the integer values corresponding to each word of the sentence. In the previous example, it would be ```[309, 12, 5, 0, 98, 12]```.\n",
    "\n",
    "Each sentence can be tranformed into a tensor using the ```preprocess_sequence``` function defined below. However, we need to create two dictionaries first:\n",
    "* a dictionary ```word_to_idx``` that maps a vocabulary word into an integer value, and\n",
    "* a dictionary ```tag_to_idx``` that maps a POS tag into an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f211a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(seq, to_idx):\n",
    "    '''\n",
    "    It processes a sequence of words/tags by transforming them into numerical values, \n",
    "    based on a {word : index} dictionary.\n",
    "    \n",
    "    Input:\n",
    "    seq: a sequence of words or POS tags in the form of a list\n",
    "    to_idx: a dictionary from word or POS tag to index value\n",
    "    \n",
    "    Output:\n",
    "    a torch tensor with numerical values that correspond to the words or POS tags of the input sequence.\n",
    "    '''\n",
    "    \n",
    "    idxs = [to_idx[w] for w in seq]  \n",
    "    \n",
    "    # You can apply further preprocessing steps if you want, \n",
    "    # such as change into lowercase or remove punctuation.\n",
    "\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:  # word has not been assigned an index yet\n",
    "            word_to_idx[word] = len(word_to_idx)  # Assign each word with a unique index\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_idx:\n",
    "            tag_to_idx[tag] = len(tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd0cf4",
   "metadata": {},
   "source": [
    "Does ```word_to_idx``` have the same length with ```vocabulary```?\n",
    "\n",
    "If not (while you have not applied preprocessing steps that have changed the length of the vocabulary, such as punctuation removal), you may have some error.\n",
    "\n",
    "If you have removed words from the vocabulary using punctuation removal or some other preprocessing step, you should then update the vocabulary.\n",
    "\n",
    "Additionally, if you remove punctuation, you should also remove it from the tag sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e20e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7752551",
   "metadata": {},
   "source": [
    "### Create the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513a13a",
   "metadata": {},
   "source": [
    "Now, we will build our LSTM network step-by-step:\n",
    "1) We will create a simple word embedding layer (dimensions: vocabulary size, embedding dimension).\n",
    "2) The output of the embedding layer is the input to our LSTM. The LSTM outputs hidden states, with dimensionality hidden_dim.\n",
    "3) We then need a linear layer to create a mapping from the hidden state space (dimensionality: hidden_dim) to the POS tag space (dimensionality: equal to the number of POS tags in our training set).\n",
    "4) Log softmax will be used in order to predict the best-scoring POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8266f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(POS_Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed53f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd9e443b",
   "metadata": {},
   "source": [
    "### Training stage\n",
    "\n",
    "We will first set the embedding and hidden dimensions, as well as the number of training epochs.\n",
    "\n",
    "We also need to choose a loss function and an [optimisation algorithm](https://pytorch.org/docs/stable/optim.html) (and its learning rate). Some options for loss function are [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and [negative log likelihood loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e59eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8   # try different values\n",
    "HIDDEN_DIM = 16    # try different values\n",
    "\n",
    "VOCAB_SIZE = len(word_to_idx) # should be the same as len(vocabulary), otherwise there is some error\n",
    "TAGS_COUNT = len(tag_to_idx)\n",
    "\n",
    "EPOCHS_NUM = 5   # try different values (maybe between 10-40)\n",
    "\n",
    "model = POS_Tagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGS_COUNT)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss() # or nn.NLLLoss() for negative log likelihood\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # stochastic gradient descent with learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10fa394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 7088.1719\n",
      "Epoch 1: Loss: 5983.7241\n",
      "Epoch 2: Loss: 5381.6421\n",
      "Epoch 3: Loss: 4965.8486\n",
      "Epoch 4: Loss: 4631.2202\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "for epoch in range(EPOCHS_NUM):\n",
    "\n",
    "    model.train()\n",
    "    for sentence, tags in training_data:\n",
    "\n",
    "        model.zero_grad() # pytorch accumulates gradients; clear them before training each instance\n",
    "\n",
    "        sentence_in = preprocess_sequence(sentence, word_to_idx) # turn sentences into tensors of word indices\n",
    "        targets = preprocess_sequence(tags, tag_to_idx) # now repeat for POS tags per sentence\n",
    "\n",
    "        predictions = model(sentence_in) # run the forward pass (defined earlier in the POS_Tagger class)\n",
    "\n",
    "        # Compute the loss, gradients, and update the parameters by calling optimizer.step()\n",
    "        loss = loss_function(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in training_data:\n",
    "            sentence_in = preprocess_sequence(sentence, word_to_idx)\n",
    "            targets = preprocess_sequence(tags, tag_to_idx)\n",
    "            y_pred = model(sentence_in)\n",
    "            loss += loss_function(y_pred, targets)\n",
    "        print(\"Epoch %d: Loss: %.4f\" % (epoch, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7dd22b",
   "metadata": {},
   "source": [
    "### Summary of the model, showing the layers, their dimensions and the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e794e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_Tagger(\n",
      "  (word_embeddings): Embedding(11993, 8)\n",
      "  (lstm): LSTM(8, 16)\n",
      "  (hidden2tag): Linear(in_features=16, out_features=46, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659fd65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "POS_Tagger                               --\n",
       "├─Embedding: 1-1                         95,944\n",
       "├─LSTM: 1-2                              1,664\n",
       "├─Linear: 1-3                            782\n",
       "=================================================================\n",
       "Total params: 98,390\n",
       "Trainable params: 98,390\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more detailed summary of the model\n",
    "# it requires installation of the torchinfo library\n",
    "# (you can install it by commenting out the next line)\n",
    "\n",
    "#!pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "251cb115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type (var_name))                  Param #\n",
      "=================================================================\n",
      "POS_Tagger (POS_Tagger)                  --\n",
      "├─Embedding (word_embeddings)            95,944\n",
      "│    └─weight                            └─95,944\n",
      "├─LSTM (lstm)                            1,664\n",
      "│    └─weight_ih_l0                      ├─512\n",
      "│    └─weight_hh_l0                      ├─1,024\n",
      "│    └─bias_ih_l0                        ├─64\n",
      "│    └─bias_hh_l0                        └─64\n",
      "├─Linear (hidden2tag)                    782\n",
      "│    └─weight                            ├─736\n",
      "│    └─bias                              └─46\n",
      "=================================================================\n",
      "Total params: 98,390\n",
      "Trainable params: 98,390\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type (var_name))                  Param #\n",
       "=================================================================\n",
       "POS_Tagger (POS_Tagger)                  --\n",
       "├─Embedding (word_embeddings)            95,944\n",
       "│    └─weight                            └─95,944\n",
       "├─LSTM (lstm)                            1,664\n",
       "│    └─weight_ih_l0                      ├─512\n",
       "│    └─weight_hh_l0                      ├─1,024\n",
       "│    └─bias_ih_l0                        ├─64\n",
       "│    └─bias_hh_l0                        └─64\n",
       "├─Linear (hidden2tag)                    782\n",
       "│    └─weight                            ├─736\n",
       "│    └─bias                              └─46\n",
       "=================================================================\n",
       "Total params: 98,390\n",
       "Trainable params: 98,390\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# even more detailed summary\n",
    "\n",
    "summary(model, verbose=2, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0147a3",
   "metadata": {},
   "source": [
    "### Compare with NLTK POS tagger\n",
    "\n",
    "We will now see how our LSTM POS tagger and the POS tagger of the NLTK library perform on a few example sentences. Note that the POS tagger of NLTK does not always predict the correct POS tag, so you should not consider its predictions as gold labels.\n",
    "\n",
    "In the training stage, we used ```tag_to_idx``` to transform POS tags into numerical values. Running our model with the example sentences as input will return values that correspond to POS tags. We should therefore change these values into POS tag names using a dictionary ```idx_to_tag```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0547de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do *not* try to make changes to the test sentences before reaching the end of the notebook\n",
    "\n",
    "test_sentences = [\n",
    "    'London is the capital of England .',\n",
    "    'Paris is the capital city of France .',\n",
    "    'They are my best friends .',\n",
    "    'This was their favorite toy .',\n",
    "    'I will be reading this book today , tomorrow and the day after .',\n",
    "    'And now for something completely different',\n",
    "    'We successfully completed the task',\n",
    "    'The company had a net loss of $ 2 million .',\n",
    "    'The discussions are still in preliminary stages , and the specific details have n\\'t been worked out',\n",
    "    'But for small American companies , it also provides a growing source of capital and even marketing help .',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52858731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a mapping (idx_to_tag) from tag ids to tag names\n",
    "# this is basically the reverse of tag_to_idx\n",
    "\n",
    "idx_to_tag = {tag_to_idx[k]:k for k in tag_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b01e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital of England .\n",
      "LSTM: ['NNP', 'VBZ', 'DT', 'NN', 'IN', 'JJ', '.']\n",
      "NLTK: ['NNP', 'VBZ', 'DT', 'NN', 'IN', 'NNP', '.']\n",
      "\n",
      "Paris is the capital city of France .\n",
      "LSTM: ['NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']\n",
      "NLTK: ['NNP', 'VBZ', 'DT', 'NN', 'NN', 'IN', 'NNP', '.']\n",
      "\n",
      "They are my best friends .\n",
      "LSTM: ['PRP', 'VBP', 'NNS', 'NNP', 'NN', '.']\n",
      "NLTK: ['PRP', 'VBP', 'PRP$', 'JJS', 'NNS', '.']\n",
      "\n",
      "This was their favorite toy .\n",
      "LSTM: ['DT', 'VBD', 'VBN', 'RB', 'NN', '.']\n",
      "NLTK: ['DT', 'VBD', 'PRP$', 'JJ', 'NN', '.']\n",
      "\n",
      "I will be reading this book today , tomorrow and the day after .\n",
      "LSTM: ['PRP', 'MD', 'VB', 'NNS', 'DT', 'NN', 'NN', ',', 'JJ', 'CC', 'DT', 'NN', 'IN', '.']\n",
      "NLTK: ['PRP', 'MD', 'VB', 'VBG', 'DT', 'NN', 'NN', ',', 'NN', 'CC', 'DT', 'NN', 'IN', '.']\n",
      "\n",
      "And now for something completely different\n",
      "LSTM: ['CC', 'RB', 'IN', 'NN', 'JJ', 'NN']\n",
      "NLTK: ['CC', 'RB', 'IN', 'NN', 'RB', 'JJ']\n",
      "\n",
      "We successfully completed the task\n",
      "LSTM: ['PRP', 'NN', 'VBD', 'DT', 'NN']\n",
      "NLTK: ['PRP', 'RB', 'VBD', 'DT', 'NN']\n",
      "\n",
      "The company had a net loss of $ 2 million .\n",
      "LSTM: ['DT', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '.']\n",
      "NLTK: ['DT', 'NN', 'VBD', 'DT', 'JJ', 'NN', 'IN', '$', 'CD', 'CD', '.']\n",
      "\n",
      "The discussions are still in preliminary stages , and the specific details have n't been worked out\n",
      "LSTM: ['DT', 'NN', 'VBP', 'JJ', 'IN', 'NNS', 'NN', ',', 'CC', 'DT', 'NN', 'NN', 'VBP', 'RB', 'VBN', 'JJ', 'IN']\n",
      "NLTK: ['DT', 'NNS', 'VBP', 'RB', 'IN', 'JJ', 'NNS', ',', 'CC', 'DT', 'NN', 'NNS', 'VBP', 'RB', 'VBN', 'VBN', 'RP']\n",
      "\n",
      "But for small American companies , it also provides a growing source of capital and even marketing help .\n",
      "LSTM: ['CC', 'IN', 'NNS', 'NNP', 'NNS', ',', 'PRP', 'RB', 'VBN', 'DT', 'NN', 'NN', 'IN', 'NN', 'CC', 'VBD', 'JJ', 'NNS', '.']\n",
      "NLTK: ['CC', 'IN', 'JJ', 'JJ', 'NNS', ',', 'PRP', 'RB', 'VBZ', 'DT', 'VBG', 'NN', 'IN', 'NN', 'CC', 'RB', 'NN', 'NN', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "for s in test_sentences:\n",
    "    sentence_in = preprocess_sequence(s.split(), word_to_idx)\n",
    "    output = model(sentence_in)\n",
    "    _, predicted_tags = output.max(dim = -1)\n",
    "\n",
    "    preds = []\n",
    "    for tag in predicted_tags:\n",
    "        preds.append(idx_to_tag[int(tag)])\n",
    "\n",
    "    print(s)\n",
    "    print('LSTM:', preds)\n",
    "    print('NLTK:', [nltk_tag[1] for nltk_tag in pos_tag(s.split())])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7793c",
   "metadata": {},
   "source": [
    "The results depend on the training parameters and hyperparameters. Remember that NLTK is not always right; for example, in the sentence \"Paris is the capital city of France .\" both the NLTK and our POS tagger agree both that 'capital' is a noun (NN), while it is an adjective (JJ).\n",
    "\n",
    "If you try to add more test sentences, you may get a KeyError. Try to think about why this happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9c0ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a92657",
   "metadata": {},
   "source": [
    "The reason why some new test sentences raise a KeyError is that they contain words that do not exist in our vocabulary, and, as a result, in our word_to_idx mapping. It is therefore impossible for our implementation to work with such cases. Can you think of possible ways to solve this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69488da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66f07179",
   "metadata": {},
   "source": [
    "This is also a good opportunity to think about our ```tag_to_idx``` mapping. Did you implement it based on the link with the POS tag list given at the beginning or based on the POS tags that exist in our training set? Are they the same? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65da5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
